{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, Convolution2D, core, BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers.core import Lambda\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import binary_crossentropy\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type ='BFC'\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'highway'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "3450\n",
      "loading done\n",
      "Saving to .npy files done.\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "3450\n",
      "loading done\n",
      "Saving to imgs_test.npy files done.\n"
     ]
    }
   ],
   "source": [
    "mydata = dataProcess(256,256, tag=tag)\n",
    "mydata.create_train_data()\n",
    "mydata.create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_batch(x, n_gpus, part):\n",
    "    \"\"\"\n",
    "    Divide the input batch into [n_gpus] slices, and obtain slice no.\n",
    "    [part].\n",
    "\n",
    "    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sh = K.shape(x)\n",
    "\n",
    "    L = sh[0] / n_gpus\n",
    "    L = tf.cast(L, tf.int32)\n",
    "\n",
    "    if part == n_gpus - 1:\n",
    "\n",
    "        return x[part*L:]\n",
    "\n",
    "    return x[part*L:int(part+1)*L]\n",
    "\n",
    "def to_multi_gpu(model, n_gpus=4):\n",
    "\n",
    "    \"\"\"Given a keras [model], return an equivalent model which parallelizes\n",
    "\n",
    "    the computation over [n_gpus] GPUs.\n",
    "\n",
    "\n",
    "\n",
    "    Each GPU gets a slice of the input batch, applies the model on that\n",
    "    slice\n",
    "\n",
    "    and later the outputs of the models are concatenated to a single\n",
    "    tensor,\n",
    "\n",
    "    hence the user sees a model that behaves the same as the original.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        x = Input(model.input_shape[1:], name=model.input_names[0])\n",
    "\n",
    "\n",
    "    towers = []\n",
    "\n",
    "    for g in range(n_gpus):\n",
    "\n",
    "        with tf.device('/gpu:' + str(g)):\n",
    "\n",
    "            slice_g = Lambda(slice_batch, lambda shape: shape, arguments={'n_gpus':n_gpus, 'part':g})(x)\n",
    "\n",
    "            towers.append(model(slice_g))\n",
    "\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        merged = merge(towers, mode='concat', concat_axis=0)\n",
    "\n",
    "    new_model = Model(input=[x], output=merged)\n",
    "    \n",
    "    funcType = type(model.save)\n",
    "    def new_save(self_,filepath, overwrite=True):\n",
    "        model.save(filepath, overwrite)\n",
    "    new_model.save=funcType(new_save, new_model)\n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-12\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myUnet(object):\n",
    "\n",
    "    def __init__(self, tp = 'highway', img_rows = 256, img_cols = 256):\n",
    "\n",
    "        self.tp = tp\n",
    "        self.type = tp\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "\n",
    "    def load_data(self):\n",
    "\n",
    "        mydata = dataProcess(self.img_rows, self.img_cols, tag = self.type)\n",
    "        imgs_train, imgs_mask_train = mydata.load_train_data()\n",
    "        imgs_test = mydata.load_test_data()\n",
    "        return imgs_train, imgs_mask_train, imgs_test\n",
    "\n",
    "    #Define the neural network\n",
    "    def get_unet(self):\n",
    "        inputs = Input((256, 256, 1))\n",
    "        #\n",
    "        conv1 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "        conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "        conv1 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv1)\n",
    "        conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "        conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        #\n",
    "        conv2 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "        conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "        conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same', kernel_initializer = 'he_normal')(conv2)\n",
    "        conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "        conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        #\n",
    "        conv3 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "        conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "        conv3 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv3)\n",
    "        conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "        conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        #\n",
    "        conv4 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "        conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "        conv4 = Convolution2D(256, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv4)\n",
    "        conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "        conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "\n",
    "        #\n",
    "        up1 = merge([UpSampling2D(size=(2, 2))(conv4), conv3], mode='concat', concat_axis=-1)\n",
    "        conv7 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(up1)\n",
    "        conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "        conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "        conv7 = Convolution2D(128, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv7)\n",
    "        conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "        conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "        #\n",
    "        up3 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=-1)\n",
    "        conv8 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer='he_normal')(up3)\n",
    "        conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "        conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "        conv8 = Convolution2D(64, 3, 3, border_mode='same', kernel_initializer='he_normal')(conv8)\n",
    "        conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "        conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "        #\n",
    "        up4 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=-1)\n",
    "        conv9 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(up4)\n",
    "        conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "        conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "        conv9 = Convolution2D(32, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "        conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "        conv9 = Convolution2D(2, 3, 3, border_mode='same', kernel_initializer = 'he_normal')(conv9)\n",
    "        #\n",
    "        conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "        model = Model(input=inputs, output=conv10)\n",
    "\n",
    "        model.compile(optimizer= Nadam(lr = 1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "#         model = to_multi_gpu(model, n_gpus=2)\n",
    "#         model.compile(optimizer= Nadam(lr = 1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print(\"loading train data\")\n",
    "        imgs_train, imgs_mask_train, imgs_test = self.load_data()\n",
    "        print(\"loading train data done\")\n",
    "        model = self.get_unet()\n",
    "        print(\"got unet\")\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint('model_weights/sentinel_unet_%s.hdf5' % self.tp, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "        print('Fitting model...')\n",
    "        model.fit(imgs_train, imgs_mask_train, batch_size=8, nb_epoch=50, verbose=1, validation_split=0.3, shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "    def predict(self):\n",
    "        print(\"loading test data\")\n",
    "        imgs_train, imgs_mask_train, imgs_test = self.load_data()\n",
    "        print(\"loading test data done\")\n",
    "        model = self.get_unet()\n",
    "        print(\"got unet\")\n",
    "        \n",
    "        model.load_weights('model_weights/sentinel_unet_%s.hdf5' % self.tp)\n",
    "        model.compile(optimizer=Nadam(lr=1e-3), loss=jaccard_coef_loss,\n",
    "                      metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "        print('Predicting...')\n",
    "        imgs_mask_test = model.predict(imgs_test, batch_size=8, verbose=1)\n",
    "        np.save('npy/imgs_mask_test_%s.npy' % self.tp, imgs_mask_test)\n",
    "\n",
    "    def save_img(self):\n",
    "\n",
    "        print(\"Array to images\")\n",
    "        imgs = np.load('npy/imgs_mask_test_%s.npy' % self.tp)\n",
    "        imgs_locations = np.load('npy/imgs_test_file_locations.npy')\n",
    "        for i in range(imgs.shape[0]):\n",
    "            img = imgs[i]\n",
    "            img[img > 0.5] = 1\n",
    "            img[img <= 0.5] = 0\n",
    "            img = array_to_img(img)\n",
    "            img_location = imgs_locations[i]\n",
    "            img_location = '/'.join(img_location.split('/')[:-1]) + '/' + img_location.split('/')[-1].split('.')[0] + '_pred_%s.png' % self.tp\n",
    "\n",
    "            img.save(img_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "------------------------------\n",
      "load train images...\n",
      "------------------------------\n",
      "------------------------------\n",
      "load test images...\n",
      "------------------------------\n",
      "loading train data done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", activation=\"relu\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:49: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:67: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:70: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"sigmoid\")`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:81: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "/usr/lib/python3.5/site-packages/ipykernel_launcher.py:99: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got unet\n",
      "Fitting model...\n",
      "Train on 2415 samples, validate on 1035 samples\n",
      "Epoch 1/50\n",
      "2408/2415 [============================>.] - ETA: 0s - loss: 4.1640 - binary_crossentropy: 0.2006 - jaccard_coef_int: 0.0351Epoch 00001: loss improved from inf to 4.16341, saving model to model_weights/sentinel_unet_highway.hdf5\n",
      "2415/2415 [==============================] - 270s 112ms/step - loss: 4.1634 - binary_crossentropy: 0.2014 - jaccard_coef_int: 0.0352 - val_loss: 5.8581 - val_binary_crossentropy: 0.1666 - val_jaccard_coef_int: 0.1428\n",
      "Epoch 2/50\n",
      "2408/2415 [============================>.] - ETA: 0s - loss: 3.5308 - binary_crossentropy: 0.3013 - jaccard_coef_int: 0.0563Epoch 00002: loss improved from 4.16341 to 3.53153, saving model to model_weights/sentinel_unet_highway.hdf5\n",
      "2415/2415 [==============================] - 262s 109ms/step - loss: 3.5315 - binary_crossentropy: 0.3013 - jaccard_coef_int: 0.0562 - val_loss: 5.2631 - val_binary_crossentropy: 0.2182 - val_jaccard_coef_int: 0.1538\n",
      "Epoch 3/50\n",
      "2408/2415 [============================>.] - ETA: 0s - loss: 3.3743 - binary_crossentropy: 0.3299 - jaccard_coef_int: 0.0634Epoch 00003: loss improved from 3.53153 to 3.37420, saving model to model_weights/sentinel_unet_highway.hdf5\n",
      "2415/2415 [==============================] - 262s 109ms/step - loss: 3.3742 - binary_crossentropy: 0.3308 - jaccard_coef_int: 0.0635 - val_loss: 4.6139 - val_binary_crossentropy: 0.2271 - val_jaccard_coef_int: 0.1576\n",
      "Epoch 4/50\n",
      "2408/2415 [============================>.] - ETA: 0s - loss: 3.2168 - binary_crossentropy: 0.3514 - jaccard_coef_int: 0.0734Epoch 00004: loss improved from 3.37420 to 3.21581, saving model to model_weights/sentinel_unet_highway.hdf5\n",
      "2415/2415 [==============================] - 262s 109ms/step - loss: 3.2158 - binary_crossentropy: 0.3514 - jaccard_coef_int: 0.0735 - val_loss: 4.3886 - val_binary_crossentropy: 0.2666 - val_jaccard_coef_int: 0.1742\n",
      "Epoch 5/50\n",
      "1960/2415 [=======================>......] - ETA: 42s - loss: 3.0912 - binary_crossentropy: 0.3651 - jaccard_coef_int: 0.0816"
     ]
    }
   ],
   "source": [
    "myunet = myUnet(tag)\n",
    "myunet.train()\n",
    "myunet.predict()\n",
    "myunet.save_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
